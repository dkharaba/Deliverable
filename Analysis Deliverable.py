# -*- coding: utf-8 -*-
"""Copy of Analysis Deliverable DM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQYBdtbC0xkxuJvtMvY0f8kKpFWfrLk5

Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries

import pandas as pd
import numpy as np
import csv
from google.colab import drive


import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix, r2_score, mean_squared_error
from statsmodels.stats.outliers_influence import variance_inflation_factor

import statsmodels.api as sm
from yellowbrick.regressor import PredictionError

import seaborn as sns
!pip install heatmapz
from heatmap import heatmap, corrplot
from sklearn.model_selection import GridSearchCV

"""Data Ingestion + Cleaning"""

# Reading in data file

drive.mount('/content/gdrive/')
ds = pd.read_excel (r'/content/gdrive/Shared drives/The Commission/combined_dataset.xlsx')

# Data cleaning
# Jason Smith
ds.iloc[182, ds.columns.get_loc('Pos')] = 'PF'
# Thon Maker
ds.iloc[250, ds.columns.get_loc('Pos')] = 'C'
# Harrison Barnes
ds.iloc[29, ds.columns.get_loc('Pos')] = 'SF'
# Wilson Chandler
ds.iloc[89, ds.columns.get_loc('Pos')] = 'SF'
# Jimmy Butler
ds.iloc[42, ds.columns.get_loc('Pos')] = 'SF'
# Wesley Matthews
ds.iloc[44, ds.columns.get_loc('Pos')] = 'SG'
# Kyle Korver
ds.iloc[150, ds.columns.get_loc('Pos')] = 'SG'
# Jonathon Simmons
ds.iloc[173, ds.columns.get_loc('Pos')] = 'SG'



# Searching for anomalies
anomaly_1 = ds.loc[ds['Pos'] == 'C-PF']
anomaly_2 = ds.loc[ds['Pos'] == 'PF-SF']
anomaly_3 = ds.loc[ds['Pos'] == 'SF-SG']
anomaly_4 = ds.loc[ds['Pos'] == 'SG-PF']
anomaly_5 = ds.loc[ds['Pos'] == 'SG-SF']
anomalies = anomaly_1, anomaly_2, anomaly_3, anomaly_4, anomaly_5
anomalies

"""Exploratory Data Analysis"""

# Gives descriptive statistics on each metric
ds.describe()

# View all possible variable correlations with Salary
correlation = ds.corr().sort_values(by='2018_2019_Salary', ascending=False)
correlation['2018_2019_Salary']


"""Variable Correlation Plot"""

#View all possible variable correlations with Salary
correlation = ds.corr(method='pearson', min_periods=1)
rs_val = correlation**2

#Generating heatmap of peason correlation values
plt.figure(figsize=(12, 12))
corrplot(correlation, size_scale=300)
plt.title("Heatmap 1 â€“ Pearson Correlation", x=-8, y=1)
plt.show()



"""Top 8 variables"""

#Finding the top 8 variables with the highest correaltion with salary 
num_vals = 9
larg = rs_val.nlargest(num_vals, '2018_2019_Salary')['2018_2019_Salary']
c = larg.index
csquared_val = ds[c].corr()**2

#generating heatmap of top 8 features correlated with salary
f, ax = plt.subplots(figsize=(12, 12))
sns.set(font_scale=1)

#Removing the upper triangle 
mask = np.zeros(csquared_val.shape, dtype=np.bool)
mask[np.triu_indices_from(mask)]= True

hm = sns.heatmap(csquared_val, cbar=True, annot=True, square=True,fmt='.2f', annot_kws={'size': 15}, yticklabels=c.values, xticklabels=c.values, mask = mask)
plt.title("Heatmap with the top 8 variables most correlated with salary", x=0.5, y=0)
plt.show()

# Pairplots(Not used in Deliverable)
sns.set()
sns.pairplot(ds[['2018_2019_Salary', 'PTS', 'FGA','FG', 'FT', 'MP', 'FTA', 'TOV', '2PA']], height = 2.5)
plt.show()

#Multi-collinearity analysis with use of VIF 

features = ds[['PTS', 'FGA','FG', 'FT', 'MP', 'FTA', 'TOV', '2PA']]
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]
vif["features"] = features.columns
vif.round(1)

#Eliminate FGA, FG and FTA and check if VIF changes
feat = ds[['PTS', 'FT', 'MP', 'TOV', '2PA']]
vif1 = pd.DataFrame()
vif1["VIF Factor"] = [variance_inflation_factor(feat.values, i) for i in range(feat.shape[1])]
vif1["features"] = feat.columns
vif1.round(1)


"""## Multiple Linear Regression"""

X = ds[['PTS', 'FT', 'MP', 'TOV', '2P']]
y = ds['2018_2019_Salary'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print(X_test.shape)
print(y_test.shape)

#Scaling data
sc = preprocessing.RobustScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

regressor = LinearRegression()  
regressor.fit(X_train, y_train)

coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  
coeff_df

y_pred = regressor.predict(X_test)
y_pred_train = regressor.predict(X_train)

df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

#Generating Bar Graph of Actual Salary vs Predicted Salary 
df = df.sample(n=50)
df.plot(kind='bar',figsize=(30,8))
plt.title('Multiple Regression: Actual Salary vs Predicted Salary')
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.xlabel('Player Index from Test Dataset')  
plt.ylabel('Salary')  
plt.show()

#Generating Scatter Plot of Actual vs Predicted salary with regression line
plt.figure(figsize=(10,10))
plt.scatter(y_test, y_pred)
plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred, 1))(np.unique(y_test)), c = 'r')
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Multiple Regression: Predicted Salary vs Actual Salary")

#Evaulation Metrics Calculation
r_2 = round((r2_score(y_test, y_pred)),3)
adj_r_2 = (1 - (1 - r_2) * ((X_train.shape[0] - 1) / 
          (X_train.shape[0] - X_train.shape[1] - 1)))
err = abs(y_pred - y_test)
mape = 100 * np.mean(err/y_test)

print('Multiple Linear Regression')
print('Mean Absolute Error Test:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error Test:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error Test:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('Normalised Root Mean Squared Error Test:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))/(np.max(y_test) - np.min(y_test)))
print('Mean Absolute Error Train:', metrics.mean_absolute_error(y_train, y_pred_train))  
print('Mean Squared Error Train:', metrics.mean_squared_error(y_train, y_pred_train))  
print('Root Mean Squared Error Train:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))
print('Normalised Root Mean Squared Error Train:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))/(np.max(y_train) - np.min(y_train)))
print('R-Squared value: ', r_2)
print('Adjusted R-Squared value: ', adj_r_2)

"""Random Forest"""

#random forest implementation
X_r = ds[['PTS', 'FT', 'MP', 'TOV', '2PA']].values
y_r = ds['2018_2019_Salary'].values

X_tr, X_te, y_tr, y_te = train_test_split(X_r, y_r, test_size=0.20, random_state=0)

print(X_tr.shape)
print(X_te.shape)
print(y_tr.shape)
print(y_te.shape)

scale = preprocessing.RobustScaler()
X_tr = scale.fit_transform(X_tr)
X_te = scale.transform(X_te)

#HyperParameter Tuning to find max_depth and n_Estimators
tune_params = [{'max_depth': [1,2,3,4,5,10, 15, 20, 50, 70], 'n_estimators': [10, 25, 50, 100, 125,150, 200, 225, 250, 300, 350, 400, 450, 500]}]
MSE_tune = ['mean_squared_error(y_te, y_predict)']
for val in MSE_tune:
    regr_tune = GridSearchCV(RandomForestRegressor(), tune_params, cv=4)
    regr_tune.fit(X_tr, y_tr)
    y_true_rf, y_predict = y_te, regr__tune.predict(X_te)

print('The best hyper-parameters for Random Forests are: ',regr_tune.best_params_)

#Fitting and predicting 
reg = RandomForestRegressor(max_depth = 2, random_state=0, n_estimators=250)
reg.fit(X_tr, y_tr)
y_p = reg.predict(X_te)
y_p_tr = reg.predict(X_tr)

#Checking for feature importance 
X_tr = pd.DataFrame(X_tr)
feature_importances = pd.DataFrame(reg.feature_importances_,
                                   index = X_tr.columns,
                                    columns=['importance']).sort_values('importance', ascending=False)
print(feature_importances)


#Creating bar graph of Actual vs Predicted Salary 
df = pd.DataFrame({'Actual': y_te.flatten(), 'Predicted': y_p.flatten()})
df1 = df.sample(n=50)
df1.plot(kind='bar',figsize=(32,10))
plt.title('Random Forest: Actual Salary vs Predicted Salary')  
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.xlabel('Player Index from Test Dataset')  
plt.ylabel('Salary')  
plt.show()


#Plotting scatter plot of actual vs predicted salary with regression line
plt.figure(figsize=(10,10))
plt.scatter(y_te, y_p)
plt.plot(np.unique(y_te), np.poly1d(np.polyfit(y_te, y_p, 1))(np.unique(y_te)), c = 'r')
plt.xlabel("Actual Salary")
plt.ylabel("Predicted Salary")
plt.title("Random Forest: Predicted Salary vs Actual Salary")


#Calculation of Evaluation Metrics
r_2_random = reg.score(X_te, y_te)
adj_r_2_random = (1 - (1 - r_2_random) * ((X_tr.shape[0] - 1) / 
          (X_tr.shape[0] - X_tr.shape[1] - 1)))
err_rand = abs(y_p - y_te)
mape_rand = 100 * np.mean(err/y_te)

print('Mean Absolute Error Test:', metrics.mean_absolute_error(y_te, y_p))
print('Mean Squared Error Test:', metrics.mean_squared_error(y_te, y_p))
print('Root Mean Squared Error Test:', np.sqrt(metrics.mean_squared_error(y_te, y_p)))
print('Normalised Root Mean Squared Error Test:', np.sqrt(metrics.mean_squared_error(y_te, y_p))/(np.max(y_te) - np.min(y_te)))

print('Mean Absolute Error Train:', metrics.mean_absolute_error(y_tr, y_p_tr))
print('Mean Squared Error Train:', metrics.mean_squared_error(y_tr, y_p_tr))
print('Root Mean Squared Error Train:', np.sqrt(metrics.mean_squared_error(y_tr, y_p_tr)))
print('Normalised Root Mean Squared Error Train:', np.sqrt(metrics.mean_squared_error(y_tr, y_p_tr))/(np.max(y_tr) - np.min(y_tr)))
print('R-Squared Value: ', r_2_random)
print('Adjusted R-Squared Value: ', adj_r_2_random)




"""Histograms with Top 5 Variables"""

# PTS

ds['PTS'].plot.hist()
plt.xlim(0)
plt.xlabel('Points Per Game')
plt.ylabel('Number of Players')
plt.title('Distribution of NBA Player Points Per Game')
plt.show()

ds['PTS'].plot.hist(density=True)
ds['PTS'].plot.density()
plt.xlim(0)
plt.xlabel('Points Per Game')
plt.title('Probability Density Distribution of NBA Player Points Per Game')
plt.show()

# FT

ds['FT'].plot.hist()
plt.xlim(0)
plt.xlabel('Free Throws Per Game')
plt.ylabel('Number of Players')
plt.title('Distribution of NBA Player Free Throws Per Game')
plt.show()

ds['FT'].plot.hist(density=True)
ds['FT'].plot.density()
plt.xlim(0)
plt.xlabel('Free Throws Per Game')
plt.title('Probability Density Distribution of NBA Player Free Throws Per Game')
plt.show()

# MP

ds['MP'].plot.hist()
plt.xlim(0)
plt.xlabel('Minutes Played Per Game')
plt.ylabel('Number of Players')
plt.title('Distribution of NBA Player Minutes Played Per Game')
plt.show()

ds['MP'].plot.hist(density=True)
ds['MP'].plot.density()
plt.xlim(0)
plt.xlabel('Minutes Played Per Game')
plt.title('Probability Density Distribution of NBA Player Minutes Played Per Game')
plt.show()

# TOV

ds['TOV'].plot.hist()
plt.xlim(0)
plt.xlabel('Turnovers Per Game')
plt.ylabel('Number of Players')
plt.title('Distribution of NBA Player Turnovers Per Game')
plt.show()

ds['TOV'].plot.hist(density=True)
ds['TOV'].plot.density()
plt.xlim(0)
plt.xlabel('Turnovers Per Game')
plt.title('Probability Density Distribution of NBA Player Turnovers Per Game')
plt.show()

# 2PA

ds['2PA'].plot.hist()
plt.xlim(0)
plt.xlabel('2-Point Attempts Per Game')
plt.ylabel('Number of Players')
plt.title('Distribution of NBA Player 2-Point Attempts Per Game')
plt.show()

ds['2PA'].plot.hist(density=True)
ds['2PA'].plot.density()
plt.xlim(0)
plt.xlabel('2-Point Attempts Per Game')
plt.title('Probability Density Distribution of NBA Player 2-Point Attempts Per Game')
plt.show()
